{
 "cells": [
  {
   "cell_type": "code",
   "id": "dd99abb2b704f1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T13:48:18.170093Z",
     "start_time": "2025-11-25T13:48:15.958556Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm  # Import tqdm for progress bars"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T13:48:18.319370Z",
     "start_time": "2025-11-25T13:48:18.260754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Paths\n",
    "train_path = r\"C:\\Users\\roeyn\\Coding_Enviroment\\NLP\\Ex1\\.venv\\train.csv\"\n",
    "val_path = r\"C:\\Users\\roeyn\\Coding_Enviroment\\NLP\\Ex1\\.venv\\validation.csv\"\n",
    "\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower() #change text to be string type and make all letters to lower case.\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) #delete any char that is not alphabetic or space.\n",
    "    return text.split() # split each word (by using space) to be a different element in the returned string.\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_csv(train_path)  # read train set\n",
    "val_df = pd.read_csv(val_path)  # read validation set"
   ],
   "id": "20eec78bac697de6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T13:48:49.328133Z",
     "start_time": "2025-11-25T13:48:31.739398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Apply preprocessing with progress bar\n",
    "print(\"Preprocessing text...\")\n",
    "tqdm.pandas(desc=\"Preprocessing Train\")\n",
    "train_df['text'] = train_df['text'].progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocessing Val\")\n",
    "val_df['text'] = val_df['text'].progress_apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"Building vocabulary...\")\n",
    "all_words = []\n",
    "for text in tqdm(train_df['text'], desc=\"Collecting words\"):\n",
    "    all_words.extend(text)\n",
    "\n",
    "word_counts = Counter(all_words)\n",
    "vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.most_common(5000)]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "\n",
    "# Word2Vec-like embedding (using co-occurrence matrix)\n",
    "def create_embeddings(texts, vocab_size, embed_dim=100, window=5):\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.randn(vocab_size, embed_dim) * 0.01\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "embeddings = create_embeddings(train_df['text'], len(vocab), embed_dim=100)\n",
    "\n",
    "\n",
    "# Convert text to sequences\n",
    "def text_to_sequence(text, word_to_idx, max_len=50):\n",
    "    seq = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in text]\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [word_to_idx['<PAD>']] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n",
    "\n",
    "\n",
    "max_len = 50\n",
    "print(\"Converting text to sequences...\")\n",
    "X_train = np.array(\n",
    "    [text_to_sequence(text, word_to_idx, max_len) for text in tqdm(train_df['text'], desc=\"Tokenizing Train\")])\n",
    "X_val = np.array([text_to_sequence(text, word_to_idx, max_len) for text in tqdm(val_df['text'], desc=\"Tokenizing Val\")])\n",
    "y_train = train_df['label'].values\n",
    "y_val = val_df['label'].values\n",
    "\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(np.clip(x, -500, 500))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# LSTM Cell Implementation\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Xavier initialization\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        self.Wf = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.Wi = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.Wc = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "        self.Wo = np.random.randn(input_size + hidden_size, hidden_size) * scale\n",
    "\n",
    "        self.bf = np.ones((1, hidden_size))  # Forget gate bias initialized to 1\n",
    "        self.bi = np.zeros((1, hidden_size))\n",
    "        self.bc = np.zeros((1, hidden_size))\n",
    "        self.bo = np.zeros((1, hidden_size))\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        concat = np.concatenate([x, h_prev], axis=1)\n",
    "\n",
    "        f = sigmoid(np.dot(concat, self.Wf) + self.bf)  # Forget gate\n",
    "        i = sigmoid(np.dot(concat, self.Wi) + self.bi)  # Input gate\n",
    "        c_tilde = tanh(np.dot(concat, self.Wc) + self.bc)  # Candidate cell state\n",
    "        o = sigmoid(np.dot(concat, self.Wo) + self.bo)  # Output gate\n",
    "\n",
    "        c = f * c_prev + i * c_tilde  # New cell state\n",
    "        h = o * tanh(c)  # New hidden state\n",
    "\n",
    "        cache = (x, h_prev, c_prev, concat, f, i, c_tilde, o, c)\n",
    "        return h, c, cache\n",
    "\n",
    "    def backward(self, dh, dc_next, cache):\n",
    "        x, h_prev, c_prev, concat, f, i, c_tilde, o, c = cache\n",
    "\n",
    "        dc = dc_next + dh * o * (1 - tanh(c) ** 2)\n",
    "\n",
    "        do = dh * tanh(c)\n",
    "        do_input = do * o * (1 - o)\n",
    "\n",
    "        dc_tilde = dc * i\n",
    "        dc_tilde_input = dc_tilde * (1 - c_tilde ** 2)\n",
    "\n",
    "        di = dc * c_tilde\n",
    "        di_input = di * i * (1 - i)\n",
    "\n",
    "        df = dc * c_prev\n",
    "        df_input = df * f * (1 - f)\n",
    "\n",
    "        dconcat = (np.dot(df_input, self.Wf.T) +\n",
    "                   np.dot(di_input, self.Wi.T) +\n",
    "                   np.dot(dc_tilde_input, self.Wc.T) +\n",
    "                   np.dot(do_input, self.Wo.T))\n",
    "\n",
    "        dx = dconcat[:, :x.shape[1]]\n",
    "        dh_prev = dconcat[:, x.shape[1]:]\n",
    "        dc_prev = dc * f\n",
    "\n",
    "        self.dWf = np.dot(concat.T, df_input)\n",
    "        self.dWi = np.dot(concat.T, di_input)\n",
    "        self.dWc = np.dot(concat.T, dc_tilde_input)\n",
    "        self.dWo = np.dot(concat.T, do_input)\n",
    "\n",
    "        self.dbf = np.sum(df_input, axis=0, keepdims=True)\n",
    "        self.dbi = np.sum(di_input, axis=0, keepdims=True)\n",
    "        self.dbc = np.sum(dc_tilde_input, axis=0, keepdims=True)\n",
    "        self.dbo = np.sum(do_input, axis=0, keepdims=True)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "# GRU Cell Implementation\n",
    "class GRUCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        scale = np.sqrt(2.0 / (input_size + hidden_size))\n",
    "        self.Wr = np.random.randn(input_size + hidden_size, hidden_size) * scale  # Reset gate\n",
    "        self.Wz = np.random.randn(input_size + hidden_size, hidden_size) * scale  # Update gate\n",
    "        self.Wh = np.random.randn(input_size + hidden_size, hidden_size) * scale  # Candidate\n",
    "\n",
    "        self.br = np.zeros((1, hidden_size))\n",
    "        self.bz = np.zeros((1, hidden_size))\n",
    "        self.bh = np.zeros((1, hidden_size))\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        concat = np.concatenate([x, h_prev], axis=1)\n",
    "\n",
    "        r = sigmoid(np.dot(concat, self.Wr) + self.br)  # Reset gate\n",
    "        z = sigmoid(np.dot(concat, self.Wz) + self.bz)  # Update gate\n",
    "\n",
    "        concat_r = np.concatenate([x, r * h_prev], axis=1)\n",
    "        h_tilde = tanh(np.dot(concat_r, self.Wh) + self.bh)  # Candidate hidden state\n",
    "\n",
    "        h = z * h_prev + (1 - z) * h_tilde  # New hidden state\n",
    "\n",
    "        cache = (x, h_prev, concat, concat_r, r, z, h_tilde)\n",
    "        return h, cache\n",
    "\n",
    "    def backward(self, dh, cache):\n",
    "        x, h_prev, concat, concat_r, r, z, h_tilde = cache\n",
    "\n",
    "        dh_tilde = dh * (1 - z)\n",
    "        dh_tilde_input = dh_tilde * (1 - h_tilde ** 2)\n",
    "\n",
    "        dz = dh * (h_prev - h_tilde)\n",
    "        dz_input = dz * z * (1 - z)\n",
    "\n",
    "        dconcat_r = np.dot(dh_tilde_input, self.Wh.T)\n",
    "        dx_from_h = dconcat_r[:, :x.shape[1]]\n",
    "        dr_h_prev = dconcat_r[:, x.shape[1]:]\n",
    "\n",
    "        dr = dr_h_prev * h_prev\n",
    "        dr_input = dr * r * (1 - r)\n",
    "\n",
    "        dconcat = np.dot(dr_input, self.Wr.T) + np.dot(dz_input, self.Wz.T)\n",
    "        dx = dconcat[:, :x.shape[1]] + dx_from_h\n",
    "        dh_prev = dconcat[:, x.shape[1]:] + dr_h_prev * r + dh * z\n",
    "\n",
    "        self.dWr = np.dot(concat.T, dr_input)\n",
    "        self.dWz = np.dot(concat.T, dz_input)\n",
    "        self.dWh = np.dot(concat_r.T, dh_tilde_input)\n",
    "\n",
    "        self.dbr = np.sum(dr_input, axis=0, keepdims=True)\n",
    "        self.dbz = np.sum(dz_input, axis=0, keepdims=True)\n",
    "        self.dbh = np.sum(dh_tilde_input, axis=0, keepdims=True)\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "# LSTM Network\n",
    "class LSTMNetwork:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.lstm_cell = LSTMCell(embed_dim, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Wy = np.random.randn(hidden_size, num_classes) * 0.01\n",
    "        self.by = np.zeros((1, num_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len = X.shape\n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "        c = np.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "        caches = []\n",
    "        for t in range(seq_len):\n",
    "            x = self.embeddings[X[:, t]]\n",
    "            h, c, cache = self.lstm_cell.forward(x, h, c)\n",
    "            caches.append(cache)\n",
    "\n",
    "        logits = np.dot(h, self.Wy) + self.by\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        return probs, (h, c, caches)\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        h, c, caches = cache\n",
    "        batch_size = dout.shape[0]\n",
    "\n",
    "        dh = np.dot(dout, self.Wy.T)\n",
    "        dc = np.zeros_like(c)\n",
    "\n",
    "        self.dWy = np.dot(h.T, dout)\n",
    "        self.dby = np.sum(dout, axis=0, keepdims=True)\n",
    "\n",
    "        for t in reversed(range(len(caches))):\n",
    "            dx, dh, dc = self.lstm_cell.backward(dh, dc, caches[t])\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.lstm_cell.Wf -= lr * self.lstm_cell.dWf\n",
    "        self.lstm_cell.Wi -= lr * self.lstm_cell.dWi\n",
    "        self.lstm_cell.Wc -= lr * self.lstm_cell.dWc\n",
    "        self.lstm_cell.Wo -= lr * self.lstm_cell.dWo\n",
    "\n",
    "        self.lstm_cell.bf -= lr * self.lstm_cell.dbf\n",
    "        self.lstm_cell.bi -= lr * self.lstm_cell.dbi\n",
    "        self.lstm_cell.bc -= lr * self.lstm_cell.dbc\n",
    "        self.lstm_cell.bo -= lr * self.lstm_cell.dbo\n",
    "\n",
    "        self.Wy -= lr * self.dWy\n",
    "        self.by -= lr * self.dby\n",
    "\n",
    "\n",
    "# GRU Network\n",
    "class GRUNetwork:\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "        self.gru_cell = GRUCell(embed_dim, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Wy = np.random.randn(hidden_size, num_classes) * 0.01\n",
    "        self.by = np.zeros((1, num_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size, seq_len = X.shape\n",
    "        h = np.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "        caches = []\n",
    "        for t in range(seq_len):\n",
    "            x = self.embeddings[X[:, t]]\n",
    "            h, cache = self.gru_cell.forward(x, h)\n",
    "            caches.append(cache)\n",
    "\n",
    "        logits = np.dot(h, self.Wy) + self.by\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        return probs, (h, caches)\n",
    "\n",
    "    def backward(self, dout, cache):\n",
    "        h, caches = cache\n",
    "\n",
    "        dh = np.dot(dout, self.Wy.T)\n",
    "\n",
    "        self.dWy = np.dot(h.T, dout)\n",
    "        self.dby = np.sum(dout, axis=0, keepdims=True)\n",
    "\n",
    "        for t in reversed(range(len(caches))):\n",
    "            dx, dh = self.gru_cell.backward(dh, caches[t])\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.gru_cell.Wr -= lr * self.gru_cell.dWr\n",
    "        self.gru_cell.Wz -= lr * self.gru_cell.dWz\n",
    "        self.gru_cell.Wh -= lr * self.gru_cell.dWh\n",
    "\n",
    "        self.gru_cell.br -= lr * self.gru_cell.dbr\n",
    "        self.gru_cell.bz -= lr * self.gru_cell.dbz\n",
    "        self.gru_cell.bh -= lr * self.gru_cell.dbh\n",
    "\n",
    "        self.Wy -= lr * self.dWy\n",
    "        self.by -= lr * self.dby\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=20, batch_size=32, lr=0.001):\n",
    "    train_losses, val_accs = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(X_train))\n",
    "        X_train_shuffled = X_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(X_train) // batch_size\n",
    "\n",
    "        # Wrap the loop with tqdm\n",
    "        progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        for i in progress_bar:\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            X_batch = X_train_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_train_shuffled[start_idx:end_idx]\n",
    "\n",
    "            # Forward pass\n",
    "            probs, cache = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = -np.mean(np.log(probs[range(batch_size), y_batch] + 1e-8))\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            dout = probs.copy()\n",
    "            dout[range(batch_size), y_batch] -= 1\n",
    "            dout /= batch_size\n",
    "\n",
    "            model.backward(dout, cache)\n",
    "            model.update(lr)\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            progress_bar.set_postfix({'loss': f'{loss:.4f}'})\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_probs, _ = model.forward(X_val)\n",
    "        val_pred = np.argmax(val_probs, axis=1)\n",
    "        val_acc = np.mean(val_pred == y_val)\n",
    "\n",
    "        train_losses.append(epoch_loss / num_batches)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} Completed. Avg Loss: {epoch_loss / num_batches:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_accs\n",
    "\n",
    "\n",
    "# Hyperparameter configurations\n",
    "lstm_configs = [\n",
    "    {'hidden_size': 64, 'lr': 0.001},\n",
    "    {'hidden_size': 128, 'lr': 0.0005},\n",
    "    {'hidden_size': 256, 'lr': 0.0001}\n",
    "]\n",
    "\n",
    "gru_configs = [\n",
    "    {'hidden_size': 64, 'lr': 0.001},\n",
    "    {'hidden_size': 128, 'lr': 0.0005},\n",
    "    {'hidden_size': 256, 'lr': 0.0001}\n",
    "]\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training LSTM Models\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lstm_results = []\n",
    "# Outer loop tqdm for configurations\n",
    "for i, config in enumerate(tqdm(lstm_configs, desc=\"LSTM Configs\")):\n",
    "    print(f\"\\nLSTM Config {i + 1}: {config}\")\n",
    "    model = LSTMNetwork(len(vocab), 100, config['hidden_size'], 6, embeddings)\n",
    "    losses, accs = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                               epochs=10, lr=config['lr'])\n",
    "    lstm_results.append({'config': config, 'val_acc': accs[-1]})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training GRU Models\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "gru_results = []\n",
    "# Outer loop tqdm for configurations\n",
    "for i, config in enumerate(tqdm(gru_configs, desc=\"GRU Configs\")):\n",
    "    print(f\"\\nGRU Config {i + 1}: {config}\")\n",
    "    model = GRUNetwork(len(vocab), 100, config['hidden_size'], 6, embeddings)\n",
    "    losses, accs = train_model(model, X_train, y_train, X_val, y_val,\n",
    "                               epochs=10, lr=config['lr'])\n",
    "    gru_results.append({'config': config, 'val_acc': accs[-1]})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Results Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nLSTM Results:\")\n",
    "for i, r in enumerate(lstm_results):\n",
    "    print(f\"Config {i + 1}: Val Acc = {r['val_acc']:.4f}\")\n",
    "\n",
    "print(\"\\nGRU Results:\")\n",
    "for i, r in enumerate(gru_results):\n",
    "    print(f\"Config {i + 1}: Val Acc = {r['val_acc']:.4f}\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing Train: 100%|██████████| 16000/16000 [00:00<00:00, 128437.53it/s]\n",
      "Preprocessing Val: 100%|██████████| 2000/2000 [00:00<00:00, 165668.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting words: 100%|██████████| 16000/16000 [00:00<00:00, 679205.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text to sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Train: 100%|██████████| 16000/16000 [00:00<00:00, 67259.39it/s] \n",
      "Tokenizing Val: 100%|██████████| 2000/2000 [00:00<00:00, 60745.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training LSTM Models\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Configs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LSTM Config 1: {'hidden_size': 64, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10:   0%|          | 0/500 [00:00<?, ?batch/s]\u001B[A\n",
      "Epoch 1/10:   0%|          | 0/500 [00:00<?, ?batch/s, loss=1.7917]\u001B[A\n",
      "Epoch 1/10:   0%|          | 1/500 [00:00<01:58,  4.22batch/s, loss=1.7917]\u001B[A\n",
      "Epoch 1/10:   0%|          | 1/500 [00:00<01:58,  4.22batch/s, loss=1.7916]\u001B[A\n",
      "Epoch 1/10:   0%|          | 2/500 [00:00<04:07,  2.01batch/s, loss=1.7916]\u001B[A\n",
      "Epoch 1/10:   0%|          | 2/500 [00:01<04:07,  2.01batch/s, loss=1.7915]\u001B[A\n",
      "Epoch 1/10:   1%|          | 3/500 [00:01<02:48,  2.95batch/s, loss=1.7915]\u001B[A\n",
      "Epoch 1/10:   1%|          | 3/500 [00:01<02:48,  2.95batch/s, loss=1.7915]\u001B[A\n",
      "Epoch 1/10:   1%|          | 4/500 [00:01<02:05,  3.94batch/s, loss=1.7915]\u001B[A\n",
      "Epoch 1/10:   1%|          | 4/500 [00:01<02:05,  3.94batch/s, loss=1.7915]\u001B[A\n",
      "Epoch 1/10:   1%|          | 4/500 [00:01<02:05,  3.94batch/s, loss=1.7912]\u001B[A\n",
      "Epoch 1/10:   1%|          | 6/500 [00:01<01:29,  5.55batch/s, loss=1.7912]\u001B[A\n",
      "Epoch 1/10:   1%|          | 6/500 [00:01<01:29,  5.55batch/s, loss=1.7909]\u001B[A\n",
      "Epoch 1/10:   1%|▏         | 7/500 [00:01<01:23,  5.88batch/s, loss=1.7909]\u001B[A\n",
      "Epoch 1/10:   1%|▏         | 7/500 [00:01<01:23,  5.88batch/s, loss=1.7918]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 8/500 [00:01<01:33,  5.28batch/s, loss=1.7918]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 8/500 [00:01<01:33,  5.28batch/s, loss=1.7913]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 9/500 [00:01<01:26,  5.71batch/s, loss=1.7913]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 9/500 [00:02<01:26,  5.71batch/s, loss=1.7911]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 10/500 [00:02<01:17,  6.29batch/s, loss=1.7911]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 10/500 [00:02<01:17,  6.29batch/s, loss=1.7913]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 11/500 [00:02<01:12,  6.74batch/s, loss=1.7913]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 11/500 [00:02<01:12,  6.74batch/s, loss=1.7915]\u001B[A\n",
      "Epoch 1/10:   2%|▏         | 11/500 [00:02<01:12,  6.74batch/s, loss=1.7914]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 13/500 [00:02<00:58,  8.26batch/s, loss=1.7914]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 13/500 [00:02<00:58,  8.26batch/s, loss=1.7912]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 14/500 [00:02<00:57,  8.46batch/s, loss=1.7912]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 14/500 [00:02<00:57,  8.46batch/s, loss=1.7908]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 15/500 [00:02<00:57,  8.40batch/s, loss=1.7908]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 15/500 [00:02<00:57,  8.40batch/s, loss=1.7904]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 15/500 [00:02<00:57,  8.40batch/s, loss=1.7905]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 17/500 [00:02<00:50,  9.60batch/s, loss=1.7905]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 17/500 [00:02<00:50,  9.60batch/s, loss=1.7907]\u001B[A\n",
      "Epoch 1/10:   3%|▎         | 17/500 [00:02<00:50,  9.60batch/s, loss=1.7910]\u001B[A\n",
      "Epoch 1/10:   4%|▍         | 19/500 [00:02<00:44, 10.78batch/s, loss=1.7910]\u001B[A\n",
      "Epoch 1/10:   4%|▍         | 19/500 [00:02<00:44, 10.78batch/s, loss=1.7899]\u001B[A\n",
      "Epoch 1/10:   4%|▍         | 19/500 [00:03<00:44, 10.78batch/s, loss=1.7898]\u001B[A\n",
      "Epoch 1/10:   4%|▍         | 21/500 [00:03<00:42, 11.14batch/s, loss=1.7898]\u001B[A\n",
      "Epoch 1/10:   4%|▍         | 21/500 [00:03<00:42, 11.14batch/s, loss=1.7905]\u001B[A\n",
      "Epoch 1/10:   4%|▍         | 21/500 [00:03<00:42, 11.14batch/s, loss=1.7906]\u001B[A\n",
      "Epoch 1/10:   5%|▍         | 23/500 [00:03<00:42, 11.14batch/s, loss=1.7906]\u001B[A\n",
      "Epoch 1/10:   5%|▍         | 23/500 [00:03<00:42, 11.14batch/s, loss=1.7897]\u001B[A\n",
      "Epoch 1/10:   5%|▍         | 23/500 [00:03<00:42, 11.14batch/s, loss=1.7901]\u001B[A\n",
      "Epoch 1/10:   5%|▌         | 25/500 [00:03<00:41, 11.59batch/s, loss=1.7901]\u001B[A\n",
      "Epoch 1/10:   5%|▌         | 25/500 [00:03<00:41, 11.59batch/s, loss=1.7905]\u001B[A\n",
      "Epoch 1/10:   5%|▌         | 25/500 [00:03<00:41, 11.59batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:   5%|▌         | 27/500 [00:03<00:38, 12.42batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:   5%|▌         | 27/500 [00:03<00:38, 12.42batch/s, loss=1.7902]\u001B[A\n",
      "Epoch 1/10:   5%|▌         | 27/500 [00:03<00:38, 12.42batch/s, loss=1.7897]\u001B[A\n",
      "Epoch 1/10:   6%|▌         | 29/500 [00:03<00:37, 12.69batch/s, loss=1.7897]\u001B[A\n",
      "Epoch 1/10:   6%|▌         | 29/500 [00:03<00:37, 12.69batch/s, loss=1.7900]\u001B[A\n",
      "Epoch 1/10:   6%|▌         | 29/500 [00:03<00:37, 12.69batch/s, loss=1.7902]\u001B[A\n",
      "Epoch 1/10:   6%|▌         | 31/500 [00:03<00:36, 12.98batch/s, loss=1.7902]\u001B[A\n",
      "Epoch 1/10:   6%|▌         | 31/500 [00:03<00:36, 12.98batch/s, loss=1.7888]\u001B[A\n",
      "Epoch 1/10:   6%|▌         | 31/500 [00:04<00:36, 12.98batch/s, loss=1.7904]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 33/500 [00:04<00:36, 12.67batch/s, loss=1.7904]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 33/500 [00:04<00:36, 12.67batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 33/500 [00:04<00:36, 12.67batch/s, loss=1.7890]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 35/500 [00:04<00:37, 12.25batch/s, loss=1.7890]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 35/500 [00:04<00:37, 12.25batch/s, loss=1.7894]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 35/500 [00:04<00:37, 12.25batch/s, loss=1.7888]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 37/500 [00:04<00:39, 11.79batch/s, loss=1.7888]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 37/500 [00:04<00:39, 11.79batch/s, loss=1.7894]\u001B[A\n",
      "Epoch 1/10:   7%|▋         | 37/500 [00:04<00:39, 11.79batch/s, loss=1.7885]\u001B[A\n",
      "Epoch 1/10:   8%|▊         | 39/500 [00:04<00:36, 12.51batch/s, loss=1.7885]\u001B[A\n",
      "Epoch 1/10:   8%|▊         | 39/500 [00:04<00:36, 12.51batch/s, loss=1.7882]\u001B[A\n",
      "Epoch 1/10:   8%|▊         | 39/500 [00:04<00:36, 12.51batch/s, loss=1.7881]\u001B[A\n",
      "Epoch 1/10:   8%|▊         | 41/500 [00:04<00:32, 13.94batch/s, loss=1.7881]\u001B[A\n",
      "Epoch 1/10:   8%|▊         | 41/500 [00:04<00:32, 13.94batch/s, loss=1.7888]\u001B[A\n",
      "Epoch 1/10:   8%|▊         | 41/500 [00:04<00:32, 13.94batch/s, loss=1.7883]\u001B[A\n",
      "Epoch 1/10:   9%|▊         | 43/500 [00:04<00:33, 13.49batch/s, loss=1.7883]\u001B[A\n",
      "Epoch 1/10:   9%|▊         | 43/500 [00:04<00:33, 13.49batch/s, loss=1.7882]\u001B[A\n",
      "Epoch 1/10:   9%|▊         | 43/500 [00:04<00:33, 13.49batch/s, loss=1.7896]\u001B[A\n",
      "Epoch 1/10:   9%|▉         | 45/500 [00:04<00:34, 13.33batch/s, loss=1.7896]\u001B[A\n",
      "Epoch 1/10:   9%|▉         | 45/500 [00:05<00:34, 13.33batch/s, loss=1.7891]\u001B[A\n",
      "Epoch 1/10:   9%|▉         | 45/500 [00:05<00:34, 13.33batch/s, loss=1.7895]\u001B[A\n",
      "Epoch 1/10:   9%|▉         | 47/500 [00:05<00:32, 13.81batch/s, loss=1.7895]\u001B[A\n",
      "Epoch 1/10:   9%|▉         | 47/500 [00:05<00:32, 13.81batch/s, loss=1.7874]\u001B[A\n",
      "Epoch 1/10:   9%|▉         | 47/500 [00:05<00:32, 13.81batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  10%|▉         | 49/500 [00:05<00:33, 13.66batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  10%|▉         | 49/500 [00:05<00:33, 13.66batch/s, loss=1.7877]\u001B[A\n",
      "Epoch 1/10:  10%|▉         | 49/500 [00:05<00:33, 13.66batch/s, loss=1.7881]\u001B[A\n",
      "Epoch 1/10:  10%|█         | 51/500 [00:05<00:32, 13.92batch/s, loss=1.7881]\u001B[A\n",
      "Epoch 1/10:  10%|█         | 51/500 [00:05<00:32, 13.92batch/s, loss=1.7890]\u001B[A\n",
      "Epoch 1/10:  10%|█         | 51/500 [00:05<00:32, 13.92batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 53/500 [00:05<00:30, 14.53batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 53/500 [00:05<00:30, 14.53batch/s, loss=1.7865]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 53/500 [00:05<00:30, 14.53batch/s, loss=1.7865]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 55/500 [00:05<00:28, 15.79batch/s, loss=1.7865]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 55/500 [00:05<00:28, 15.79batch/s, loss=1.7906]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 55/500 [00:05<00:28, 15.79batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:  11%|█         | 55/500 [00:05<00:28, 15.79batch/s, loss=1.7893]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 58/500 [00:05<00:25, 17.49batch/s, loss=1.7893]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 58/500 [00:05<00:25, 17.49batch/s, loss=1.7874]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 58/500 [00:05<00:25, 17.49batch/s, loss=1.7863]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 58/500 [00:05<00:25, 17.49batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 61/500 [00:05<00:23, 18.73batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 61/500 [00:05<00:23, 18.73batch/s, loss=1.7871]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 61/500 [00:05<00:23, 18.73batch/s, loss=1.7884]\u001B[A\n",
      "Epoch 1/10:  12%|█▏        | 61/500 [00:06<00:23, 18.73batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  13%|█▎        | 64/500 [00:06<00:23, 18.47batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  13%|█▎        | 64/500 [00:06<00:23, 18.47batch/s, loss=1.7881]\u001B[A\n",
      "Epoch 1/10:  13%|█▎        | 64/500 [00:06<00:23, 18.47batch/s, loss=1.7867]\u001B[A\n",
      "Epoch 1/10:  13%|█▎        | 66/500 [00:06<00:23, 18.18batch/s, loss=1.7867]\u001B[A\n",
      "Epoch 1/10:  13%|█▎        | 66/500 [00:06<00:23, 18.18batch/s, loss=1.7856]\u001B[A\n",
      "Epoch 1/10:  13%|█▎        | 66/500 [00:06<00:23, 18.18batch/s, loss=1.7864]\u001B[A\n",
      "Epoch 1/10:  14%|█▎        | 68/500 [00:06<00:23, 18.09batch/s, loss=1.7864]\u001B[A\n",
      "Epoch 1/10:  14%|█▎        | 68/500 [00:06<00:23, 18.09batch/s, loss=1.7868]\u001B[A\n",
      "Epoch 1/10:  14%|█▎        | 68/500 [00:06<00:23, 18.09batch/s, loss=1.7899]\u001B[A\n",
      "Epoch 1/10:  14%|█▍        | 70/500 [00:06<00:27, 15.69batch/s, loss=1.7899]\u001B[A\n",
      "Epoch 1/10:  14%|█▍        | 70/500 [00:06<00:27, 15.69batch/s, loss=1.7854]\u001B[A\n",
      "Epoch 1/10:  14%|█▍        | 70/500 [00:06<00:27, 15.69batch/s, loss=1.7870]\u001B[A\n",
      "Epoch 1/10:  14%|█▍        | 72/500 [00:06<00:27, 15.74batch/s, loss=1.7870]\u001B[A\n",
      "Epoch 1/10:  14%|█▍        | 72/500 [00:06<00:27, 15.74batch/s, loss=1.7841]\u001B[A\n",
      "Epoch 1/10:  14%|█▍        | 72/500 [00:06<00:27, 15.74batch/s, loss=1.7872]\u001B[A\n",
      "Epoch 1/10:  15%|█▍        | 74/500 [00:06<00:28, 15.17batch/s, loss=1.7872]\u001B[A\n",
      "Epoch 1/10:  15%|█▍        | 74/500 [00:06<00:28, 15.17batch/s, loss=1.7853]\u001B[A\n",
      "Epoch 1/10:  15%|█▍        | 74/500 [00:06<00:28, 15.17batch/s, loss=1.7854]\u001B[A\n",
      "Epoch 1/10:  15%|█▌        | 76/500 [00:06<00:28, 14.89batch/s, loss=1.7854]\u001B[A\n",
      "Epoch 1/10:  15%|█▌        | 76/500 [00:06<00:28, 14.89batch/s, loss=1.7865]\u001B[A\n",
      "Epoch 1/10:  15%|█▌        | 76/500 [00:06<00:28, 14.89batch/s, loss=1.7877]\u001B[A\n",
      "Epoch 1/10:  16%|█▌        | 78/500 [00:06<00:29, 14.18batch/s, loss=1.7877]\u001B[A\n",
      "Epoch 1/10:  16%|█▌        | 78/500 [00:07<00:29, 14.18batch/s, loss=1.7837]\u001B[A\n",
      "Epoch 1/10:  16%|█▌        | 78/500 [00:07<00:29, 14.18batch/s, loss=1.7860]\u001B[A\n",
      "Epoch 1/10:  16%|█▌        | 80/500 [00:07<00:30, 13.83batch/s, loss=1.7860]\u001B[A\n",
      "Epoch 1/10:  16%|█▌        | 80/500 [00:07<00:30, 13.83batch/s, loss=1.7893]\u001B[A\n",
      "Epoch 1/10:  16%|█▌        | 80/500 [00:07<00:30, 13.83batch/s, loss=1.7854]\u001B[A\n",
      "Epoch 1/10:  16%|█▋        | 82/500 [00:07<00:29, 14.16batch/s, loss=1.7854]\u001B[A\n",
      "Epoch 1/10:  16%|█▋        | 82/500 [00:07<00:29, 14.16batch/s, loss=1.7839]\u001B[A\n",
      "Epoch 1/10:  16%|█▋        | 82/500 [00:07<00:29, 14.16batch/s, loss=1.7866]\u001B[A\n",
      "Epoch 1/10:  17%|█▋        | 84/500 [00:07<00:31, 13.36batch/s, loss=1.7866]\u001B[A\n",
      "Epoch 1/10:  17%|█▋        | 84/500 [00:07<00:31, 13.36batch/s, loss=1.7855]\u001B[A\n",
      "Epoch 1/10:  17%|█▋        | 84/500 [00:07<00:31, 13.36batch/s, loss=1.7839]\u001B[A\n",
      "Epoch 1/10:  17%|█▋        | 86/500 [00:07<00:28, 14.43batch/s, loss=1.7839]\u001B[A\n",
      "Epoch 1/10:  17%|█▋        | 86/500 [00:07<00:28, 14.43batch/s, loss=1.7846]\u001B[A\n",
      "Epoch 1/10:  17%|█▋        | 86/500 [00:07<00:28, 14.43batch/s, loss=1.7861]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 88/500 [00:07<00:26, 15.67batch/s, loss=1.7861]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 88/500 [00:07<00:26, 15.67batch/s, loss=1.7853]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 88/500 [00:07<00:26, 15.67batch/s, loss=1.7836]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 88/500 [00:07<00:26, 15.67batch/s, loss=1.7820]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 91/500 [00:07<00:24, 16.47batch/s, loss=1.7820]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 91/500 [00:07<00:24, 16.47batch/s, loss=1.7884]\u001B[A\n",
      "Epoch 1/10:  18%|█▊        | 91/500 [00:07<00:24, 16.47batch/s, loss=1.7831]\u001B[A\n",
      "Epoch 1/10:  19%|█▊        | 93/500 [00:07<00:24, 16.81batch/s, loss=1.7831]\u001B[A\n",
      "Epoch 1/10:  19%|█▊        | 93/500 [00:07<00:24, 16.81batch/s, loss=1.7854]\u001B[A\n",
      "Epoch 1/10:  19%|█▊        | 93/500 [00:08<00:24, 16.81batch/s, loss=1.7850]\u001B[A\n",
      "Epoch 1/10:  19%|█▊        | 93/500 [00:08<00:24, 16.81batch/s, loss=1.7852]\u001B[A\n",
      "Epoch 1/10:  19%|█▉        | 96/500 [00:08<00:22, 17.82batch/s, loss=1.7852]\u001B[A\n",
      "Epoch 1/10:  19%|█▉        | 96/500 [00:08<00:22, 17.82batch/s, loss=1.7870]\u001B[A\n",
      "Epoch 1/10:  19%|█▉        | 96/500 [00:08<00:22, 17.82batch/s, loss=1.7846]\u001B[A\n",
      "Epoch 1/10:  20%|█▉        | 98/500 [00:08<00:23, 17.22batch/s, loss=1.7846]\u001B[A\n",
      "Epoch 1/10:  20%|█▉        | 98/500 [00:08<00:23, 17.22batch/s, loss=1.7844]\u001B[A\n",
      "Epoch 1/10:  20%|█▉        | 98/500 [00:08<00:23, 17.22batch/s, loss=1.7842]\u001B[A\n",
      "Epoch 1/10:  20%|██        | 100/500 [00:08<00:23, 17.34batch/s, loss=1.7842]\u001B[A\n",
      "Epoch 1/10:  20%|██        | 100/500 [00:08<00:23, 17.34batch/s, loss=1.7866]\u001B[A\n",
      "Epoch 1/10:  20%|██        | 100/500 [00:08<00:23, 17.34batch/s, loss=1.7825]\u001B[A\n",
      "Epoch 1/10:  20%|██        | 102/500 [00:08<00:22, 17.38batch/s, loss=1.7825]\u001B[A\n",
      "Epoch 1/10:  20%|██        | 102/500 [00:08<00:22, 17.38batch/s, loss=1.7839]\u001B[A\n",
      "Epoch 1/10:  20%|██        | 102/500 [00:08<00:22, 17.38batch/s, loss=1.7862]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 104/500 [00:08<00:22, 17.64batch/s, loss=1.7862]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 104/500 [00:08<00:22, 17.64batch/s, loss=1.7845]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 104/500 [00:08<00:22, 17.64batch/s, loss=1.7827]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 106/500 [00:08<00:21, 18.23batch/s, loss=1.7827]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 106/500 [00:08<00:21, 18.23batch/s, loss=1.7822]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 106/500 [00:08<00:21, 18.23batch/s, loss=1.7830]\u001B[A\n",
      "Epoch 1/10:  21%|██        | 106/500 [00:08<00:21, 18.23batch/s, loss=1.7838]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 109/500 [00:08<00:20, 19.26batch/s, loss=1.7838]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 109/500 [00:08<00:20, 19.26batch/s, loss=1.7841]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 109/500 [00:08<00:20, 19.26batch/s, loss=1.7827]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 109/500 [00:08<00:20, 19.26batch/s, loss=1.7865]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 112/500 [00:08<00:19, 19.51batch/s, loss=1.7865]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 112/500 [00:08<00:19, 19.51batch/s, loss=1.7901]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 112/500 [00:09<00:19, 19.51batch/s, loss=1.7830]\u001B[A\n",
      "Epoch 1/10:  22%|██▏       | 112/500 [00:09<00:19, 19.51batch/s, loss=1.7860]\u001B[A\n",
      "Epoch 1/10:  23%|██▎       | 115/500 [00:09<00:19, 19.53batch/s, loss=1.7860]\u001B[A\n",
      "Epoch 1/10:  23%|██▎       | 115/500 [00:09<00:19, 19.53batch/s, loss=1.7825]\u001B[A\n",
      "Epoch 1/10:  23%|██▎       | 115/500 [00:09<00:19, 19.53batch/s, loss=1.7826]\u001B[A\n",
      "Epoch 1/10:  23%|██▎       | 117/500 [00:09<00:20, 19.08batch/s, loss=1.7826]\u001B[A\n",
      "Epoch 1/10:  23%|██▎       | 117/500 [00:09<00:20, 19.08batch/s, loss=1.7835]\u001B[A\n",
      "Epoch 1/10:  23%|██▎       | 117/500 [00:09<00:20, 19.08batch/s, loss=1.7848]\u001B[A\n",
      "Epoch 1/10:  24%|██▍       | 119/500 [00:09<00:20, 18.57batch/s, loss=1.7848]\u001B[A\n",
      "Epoch 1/10:  24%|██▍       | 119/500 [00:09<00:20, 18.57batch/s, loss=1.7856]\u001B[A\n",
      "Epoch 1/10:  24%|██▍       | 119/500 [00:09<00:20, 18.57batch/s, loss=1.7837]\u001B[A\n",
      "Epoch 1/10:  24%|██▍       | 121/500 [00:09<00:20, 18.52batch/s, loss=1.7837]\u001B[A\n",
      "Epoch 1/10:  24%|██▍       | 121/500 [00:09<00:20, 18.52batch/s, loss=1.7882]\u001B[A\n",
      "Epoch 1/10:  24%|██▍       | 121/500 [00:09<00:20, 18.52batch/s, loss=1.7848]\u001B[A\n",
      "Epoch 1/10:  25%|██▍       | 123/500 [00:09<00:20, 18.40batch/s, loss=1.7848]\u001B[A\n",
      "Epoch 1/10:  25%|██▍       | 123/500 [00:09<00:20, 18.40batch/s, loss=1.7829]\u001B[A\n",
      "Epoch 1/10:  25%|██▍       | 123/500 [00:09<00:20, 18.40batch/s, loss=1.7858]\u001B[A\n",
      "Epoch 1/10:  25%|██▌       | 125/500 [00:09<00:19, 18.79batch/s, loss=1.7858]\u001B[A\n",
      "Epoch 1/10:  25%|██▌       | 125/500 [00:09<00:19, 18.79batch/s, loss=1.7845]\u001B[A\n",
      "Epoch 1/10:  25%|██▌       | 125/500 [00:09<00:19, 18.79batch/s, loss=1.7822]\u001B[A\n",
      "Epoch 1/10:  25%|██▌       | 125/500 [00:09<00:19, 18.79batch/s, loss=1.7764]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 128/500 [00:09<00:19, 19.56batch/s, loss=1.7764]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 128/500 [00:09<00:19, 19.56batch/s, loss=1.7870]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 128/500 [00:09<00:19, 19.56batch/s, loss=1.7886]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 128/500 [00:09<00:19, 19.56batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 131/500 [00:09<00:18, 20.23batch/s, loss=1.7889]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 131/500 [00:09<00:18, 20.23batch/s, loss=1.7805]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 131/500 [00:10<00:18, 20.23batch/s, loss=1.7850]\u001B[A\n",
      "Epoch 1/10:  26%|██▌       | 131/500 [00:10<00:18, 20.23batch/s, loss=1.7844]\u001B[A\n",
      "Epoch 1/10:  27%|██▋       | 134/500 [00:10<00:18, 19.80batch/s, loss=1.7844]\u001B[A\n",
      "Epoch 1/10:  27%|██▋       | 134/500 [00:10<00:18, 19.80batch/s, loss=1.7833]\u001B[A\n",
      "Epoch 1/10:  27%|██▋       | 134/500 [00:10<00:18, 19.80batch/s, loss=1.7807]\u001B[A\n",
      "Epoch 1/10:  27%|██▋       | 136/500 [00:10<00:21, 17.03batch/s, loss=1.7807]\u001B[A\n",
      "Epoch 1/10:  27%|██▋       | 136/500 [00:10<00:21, 17.03batch/s, loss=1.7799]\u001B[A\n",
      "Epoch 1/10:  27%|██▋       | 136/500 [00:10<00:21, 17.03batch/s, loss=1.7764]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 138/500 [00:10<00:20, 17.34batch/s, loss=1.7764]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 138/500 [00:10<00:20, 17.34batch/s, loss=1.7794]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 138/500 [00:10<00:20, 17.34batch/s, loss=1.7843]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 140/500 [00:10<00:21, 17.13batch/s, loss=1.7843]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 140/500 [00:10<00:21, 17.13batch/s, loss=1.7816]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 140/500 [00:10<00:21, 17.13batch/s, loss=1.7804]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 142/500 [00:10<00:20, 17.65batch/s, loss=1.7804]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 142/500 [00:10<00:20, 17.65batch/s, loss=1.7792]\u001B[A\n",
      "Epoch 1/10:  28%|██▊       | 142/500 [00:10<00:20, 17.65batch/s, loss=1.7816]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 144/500 [00:10<00:19, 18.16batch/s, loss=1.7816]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 144/500 [00:10<00:19, 18.16batch/s, loss=1.7807]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 144/500 [00:10<00:19, 18.16batch/s, loss=1.7803]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 144/500 [00:10<00:19, 18.16batch/s, loss=1.7861]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 147/500 [00:10<00:18, 19.05batch/s, loss=1.7861]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 147/500 [00:10<00:18, 19.05batch/s, loss=1.7815]\u001B[A\n",
      "Epoch 1/10:  29%|██▉       | 147/500 [00:10<00:18, 19.05batch/s, loss=1.7827]\u001B[A\n",
      "Epoch 1/10:  30%|██▉       | 149/500 [00:10<00:18, 18.97batch/s, loss=1.7827]\u001B[A\n",
      "Epoch 1/10:  30%|██▉       | 149/500 [00:11<00:18, 18.97batch/s, loss=1.7823]\u001B[A\n",
      "Epoch 1/10:  30%|██▉       | 149/500 [00:11<00:18, 18.97batch/s, loss=1.7798]\u001B[A\n",
      "Epoch 1/10:  30%|███       | 151/500 [00:11<00:18, 18.75batch/s, loss=1.7798]\u001B[A\n",
      "Epoch 1/10:  30%|███       | 151/500 [00:11<00:18, 18.75batch/s, loss=1.7769]\u001B[A\n",
      "Epoch 1/10:  30%|███       | 151/500 [00:11<00:18, 18.75batch/s, loss=1.7794]\u001B[A\n",
      "Epoch 1/10:  31%|███       | 153/500 [00:11<00:18, 18.35batch/s, loss=1.7794]\u001B[A\n",
      "Epoch 1/10:  31%|███       | 153/500 [00:11<00:18, 18.35batch/s, loss=1.7790]\u001B[A\n",
      "Epoch 1/10:  31%|███       | 153/500 [00:11<00:18, 18.35batch/s, loss=1.7759]\u001B[A\n",
      "Epoch 1/10:  31%|███       | 155/500 [00:11<00:19, 18.09batch/s, loss=1.7759]\u001B[A\n",
      "Epoch 1/10:  31%|███       | 155/500 [00:11<00:19, 18.09batch/s, loss=1.7795]\u001B[A\n",
      "Epoch 1/10:  31%|███       | 155/500 [00:11<00:19, 18.09batch/s, loss=1.7819]\u001B[A\n",
      "Epoch 1/10:  31%|███▏      | 157/500 [00:11<00:18, 18.37batch/s, loss=1.7819]\u001B[A\n",
      "Epoch 1/10:  31%|███▏      | 157/500 [00:11<00:18, 18.37batch/s, loss=1.7800]\u001B[A\n",
      "Epoch 1/10:  31%|███▏      | 157/500 [00:11<00:18, 18.37batch/s, loss=1.7859]\u001B[A\n",
      "Epoch 1/10:  32%|███▏      | 159/500 [00:11<00:18, 18.13batch/s, loss=1.7859]\u001B[A\n",
      "Epoch 1/10:  32%|███▏      | 159/500 [00:11<00:18, 18.13batch/s, loss=1.7764]\u001B[A\n",
      "Epoch 1/10:  32%|███▏      | 159/500 [00:11<00:18, 18.13batch/s, loss=1.7843]\u001B[A\n",
      "Epoch 1/10:  32%|███▏      | 161/500 [00:11<00:19, 17.61batch/s, loss=1.7843]\u001B[A\n",
      "Epoch 1/10:  32%|███▏      | 161/500 [00:11<00:19, 17.61batch/s, loss=1.7801]\u001B[A\n",
      "Epoch 1/10:  32%|███▏      | 161/500 [00:11<00:19, 17.61batch/s, loss=1.7795]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 163/500 [00:11<00:18, 18.20batch/s, loss=1.7795]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 163/500 [00:11<00:18, 18.20batch/s, loss=1.7805]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 163/500 [00:11<00:18, 18.20batch/s, loss=1.7769]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 165/500 [00:11<00:18, 18.56batch/s, loss=1.7769]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 165/500 [00:11<00:18, 18.56batch/s, loss=1.7779]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 165/500 [00:11<00:18, 18.56batch/s, loss=1.7823]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 167/500 [00:11<00:18, 18.12batch/s, loss=1.7823]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 167/500 [00:11<00:18, 18.12batch/s, loss=1.7779]\u001B[A\n",
      "Epoch 1/10:  33%|███▎      | 167/500 [00:12<00:18, 18.12batch/s, loss=1.7813]\u001B[A\n",
      "Epoch 1/10:  34%|███▍      | 169/500 [00:12<00:18, 18.24batch/s, loss=1.7813]\u001B[A\n",
      "Epoch 1/10:  34%|███▍      | 169/500 [00:12<00:18, 18.24batch/s, loss=1.7753]\u001B[A\n",
      "Epoch 1/10:  34%|███▍      | 169/500 [00:12<00:18, 18.24batch/s, loss=1.7874]\u001B[A\n",
      "Epoch 1/10:  34%|███▍      | 171/500 [00:12<00:18, 17.48batch/s, loss=1.7874]\u001B[A\n",
      "Epoch 1/10:  34%|███▍      | 171/500 [00:12<00:18, 17.48batch/s, loss=1.7842]\u001B[A\n",
      "Epoch 1/10:  34%|███▍      | 171/500 [00:12<00:18, 17.48batch/s, loss=1.7787]\u001B[A\n",
      "Epoch 1/10:  35%|███▍      | 173/500 [00:12<00:18, 17.64batch/s, loss=1.7787]\u001B[A\n",
      "Epoch 1/10:  35%|███▍      | 173/500 [00:12<00:18, 17.64batch/s, loss=1.7788]\u001B[A\n",
      "Epoch 1/10:  35%|███▍      | 173/500 [00:12<00:18, 17.64batch/s, loss=1.7783]\u001B[A\n",
      "Epoch 1/10:  35%|███▌      | 175/500 [00:12<00:18, 17.14batch/s, loss=1.7783]\u001B[A\n",
      "Epoch 1/10:  35%|███▌      | 175/500 [00:12<00:18, 17.14batch/s, loss=1.7766]\u001B[A\n",
      "Epoch 1/10:  35%|███▌      | 175/500 [00:12<00:18, 17.14batch/s, loss=1.7790]\u001B[A\n",
      "Epoch 1/10:  35%|███▌      | 177/500 [00:12<00:18, 17.15batch/s, loss=1.7790]\u001B[A\n",
      "Epoch 1/10:  35%|███▌      | 177/500 [00:12<00:18, 17.15batch/s, loss=1.7797]\u001B[A\n",
      "Epoch 1/10:  35%|███▌      | 177/500 [00:12<00:18, 17.15batch/s, loss=1.7775]\u001B[A\n",
      "Epoch 1/10:  36%|███▌      | 179/500 [00:12<00:18, 17.73batch/s, loss=1.7775]\u001B[A\n",
      "Epoch 1/10:  36%|███▌      | 179/500 [00:12<00:18, 17.73batch/s, loss=1.7860]\u001B[A\n",
      "Epoch 1/10:  36%|███▌      | 179/500 [00:12<00:18, 17.73batch/s, loss=1.7776]\u001B[A\n",
      "Epoch 1/10:  36%|███▌      | 179/500 [00:12<00:18, 17.73batch/s, loss=1.7759]\u001B[A\n",
      "Epoch 1/10:  36%|███▋      | 182/500 [00:12<00:16, 18.72batch/s, loss=1.7759]\u001B[A\n",
      "Epoch 1/10:  36%|███▋      | 182/500 [00:12<00:16, 18.72batch/s, loss=1.7799]\u001B[A\n",
      "Epoch 1/10:  36%|███▋      | 182/500 [00:12<00:16, 18.72batch/s, loss=1.7796]\u001B[A\n",
      "Epoch 1/10:  36%|███▋      | 182/500 [00:12<00:16, 18.72batch/s, loss=1.7890]\u001B[A\n",
      "Epoch 1/10:  37%|███▋      | 185/500 [00:12<00:16, 19.25batch/s, loss=1.7890]\u001B[A\n",
      "Epoch 1/10:  37%|███▋      | 185/500 [00:12<00:16, 19.25batch/s, loss=1.7817]\u001B[A\n",
      "Epoch 1/10:  37%|███▋      | 185/500 [00:13<00:16, 19.25batch/s, loss=1.7800]\u001B[A\n",
      "Epoch 1/10:  37%|███▋      | 187/500 [00:13<00:16, 18.86batch/s, loss=1.7800]\u001B[A\n",
      "Epoch 1/10:  37%|███▋      | 187/500 [00:13<00:16, 18.86batch/s, loss=1.7786]\u001B[A\n",
      "Epoch 1/10:  37%|███▋      | 187/500 [00:13<00:16, 18.86batch/s, loss=1.7794]\u001B[A\n",
      "Epoch 1/10:  38%|███▊      | 189/500 [00:13<00:17, 18.07batch/s, loss=1.7794]\u001B[A\n",
      "Epoch 1/10:  38%|███▊      | 189/500 [00:13<00:17, 18.07batch/s, loss=1.7834]\u001B[A\n",
      "Epoch 1/10:  38%|███▊      | 189/500 [00:13<00:17, 18.07batch/s, loss=1.7826]\u001B[A\n",
      "Epoch 1/10:  38%|███▊      | 191/500 [00:13<00:17, 17.58batch/s, loss=1.7826]\u001B[A\n",
      "Epoch 1/10:  38%|███▊      | 191/500 [00:13<00:17, 17.58batch/s, loss=1.7802]\u001B[A\n",
      "Epoch 1/10:  38%|███▊      | 191/500 [00:13<00:17, 17.58batch/s, loss=1.7758]\u001B[A\n",
      "Epoch 1/10:  39%|███▊      | 193/500 [00:13<00:17, 17.62batch/s, loss=1.7758]\u001B[A\n",
      "Epoch 1/10:  39%|███▊      | 193/500 [00:13<00:17, 17.62batch/s, loss=1.7839]\u001B[A\n",
      "Epoch 1/10:  39%|███▊      | 193/500 [00:13<00:17, 17.62batch/s, loss=1.7813]\u001B[A\n",
      "Epoch 1/10:  39%|███▉      | 195/500 [00:13<00:17, 17.32batch/s, loss=1.7813]\u001B[A\n",
      "Epoch 1/10:  39%|███▉      | 195/500 [00:13<00:17, 17.32batch/s, loss=1.7798]\u001B[A\n",
      "Epoch 1/10:  39%|███▉      | 195/500 [00:13<00:17, 17.32batch/s, loss=1.7834]\u001B[A\n",
      "Epoch 1/10:  39%|███▉      | 197/500 [00:13<00:16, 17.86batch/s, loss=1.7834]\u001B[A\n",
      "Epoch 1/10:  39%|███▉      | 197/500 [00:13<00:16, 17.86batch/s, loss=1.7784]\u001B[A\n",
      "Epoch 1/10:  39%|███▉      | 197/500 [00:13<00:16, 17.86batch/s, loss=1.7824]\u001B[A\n",
      "Epoch 1/10:  40%|███▉      | 199/500 [00:13<00:16, 18.31batch/s, loss=1.7824]\u001B[A\n",
      "Epoch 1/10:  40%|███▉      | 199/500 [00:13<00:16, 18.31batch/s, loss=1.7778]\u001B[A\n",
      "Epoch 1/10:  40%|███▉      | 199/500 [00:13<00:16, 18.31batch/s, loss=1.7770]\u001B[A\n",
      "Epoch 1/10:  40%|███▉      | 199/500 [00:13<00:16, 18.31batch/s, loss=1.7768]\u001B[A\n",
      "Epoch 1/10:  40%|████      | 202/500 [00:13<00:15, 19.19batch/s, loss=1.7768]\u001B[A\n",
      "Epoch 1/10:  40%|████      | 202/500 [00:13<00:15, 19.19batch/s, loss=1.7797]\u001B[A\n",
      "Epoch 1/10:  40%|████      | 202/500 [00:14<00:15, 19.19batch/s, loss=1.7779]\u001B[A\n",
      "Epoch 1/10:  41%|████      | 204/500 [00:14<00:16, 17.83batch/s, loss=1.7779]\u001B[A\n",
      "Epoch 1/10:  41%|████      | 204/500 [00:14<00:16, 17.83batch/s, loss=1.7780]\u001B[A\n",
      "Epoch 1/10:  41%|████      | 204/500 [00:14<00:16, 17.83batch/s, loss=1.7732]\u001B[A\n",
      "Epoch 1/10:  41%|████      | 206/500 [00:14<00:18, 15.57batch/s, loss=1.7732]\u001B[A\n",
      "Epoch 1/10:  41%|████      | 206/500 [00:14<00:18, 15.57batch/s, loss=1.7755]\u001B[A\n",
      "Epoch 1/10:  41%|████      | 206/500 [00:14<00:18, 15.57batch/s, loss=1.7802]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 208/500 [00:14<00:21, 13.57batch/s, loss=1.7802]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 208/500 [00:14<00:21, 13.57batch/s, loss=1.7776]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 208/500 [00:14<00:21, 13.57batch/s, loss=1.7804]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 210/500 [00:14<00:21, 13.41batch/s, loss=1.7804]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 210/500 [00:14<00:21, 13.41batch/s, loss=1.7741]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 210/500 [00:14<00:21, 13.41batch/s, loss=1.7812]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 212/500 [00:14<00:20, 14.31batch/s, loss=1.7812]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 212/500 [00:14<00:20, 14.31batch/s, loss=1.7744]\u001B[A\n",
      "Epoch 1/10:  42%|████▏     | 212/500 [00:14<00:20, 14.31batch/s, loss=1.7793]\u001B[A\n",
      "Epoch 1/10:  43%|████▎     | 214/500 [00:14<00:19, 14.96batch/s, loss=1.7793]\u001B[A\n",
      "Epoch 1/10:  43%|████▎     | 214/500 [00:14<00:19, 14.96batch/s, loss=1.7766]\u001B[A\n",
      "Epoch 1/10:  43%|████▎     | 214/500 [00:14<00:19, 14.96batch/s, loss=1.7704]\u001B[A\n",
      "Epoch 1/10:  43%|████▎     | 216/500 [00:14<00:18, 15.36batch/s, loss=1.7704]\u001B[A\n",
      "Epoch 1/10:  43%|████▎     | 216/500 [00:14<00:18, 15.36batch/s, loss=1.7820]\u001B[A\n",
      "Epoch 1/10:  43%|████▎     | 216/500 [00:15<00:18, 15.36batch/s, loss=1.7718]\u001B[A\n",
      "Epoch 1/10:  44%|████▎     | 218/500 [00:15<00:18, 15.38batch/s, loss=1.7718]\u001B[A\n",
      "Epoch 1/10:  44%|████▎     | 218/500 [00:15<00:18, 15.38batch/s, loss=1.7736]\u001B[A\n",
      "Epoch 1/10:  44%|████▎     | 218/500 [00:15<00:18, 15.38batch/s, loss=1.7729]\u001B[A\n",
      "Epoch 1/10:  44%|████▍     | 220/500 [00:15<00:18, 15.27batch/s, loss=1.7729]\u001B[A\n",
      "Epoch 1/10:  44%|████▍     | 220/500 [00:15<00:18, 15.27batch/s, loss=1.7793]\u001B[A\n",
      "Epoch 1/10:  44%|████▍     | 220/500 [00:15<00:18, 15.27batch/s, loss=1.7768]\u001B[A\n",
      "Epoch 1/10:  44%|████▍     | 222/500 [00:15<00:19, 14.52batch/s, loss=1.7768]\u001B[A\n",
      "LSTM Configs:   0%|          | 0/3 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 372\u001B[39m\n\u001B[32m    370\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mLSTM Config \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    371\u001B[39m     model = LSTMNetwork(\u001B[38;5;28mlen\u001B[39m(vocab), \u001B[32m100\u001B[39m, config[\u001B[33m'\u001B[39m\u001B[33mhidden_size\u001B[39m\u001B[33m'\u001B[39m], \u001B[32m6\u001B[39m, embeddings)\n\u001B[32m--> \u001B[39m\u001B[32m372\u001B[39m     losses, accs = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    373\u001B[39m \u001B[43m                               \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    374\u001B[39m     lstm_results.append({\u001B[33m'\u001B[39m\u001B[33mconfig\u001B[39m\u001B[33m'\u001B[39m: config, \u001B[33m'\u001B[39m\u001B[33mval_acc\u001B[39m\u001B[33m'\u001B[39m: accs[-\u001B[32m1\u001B[39m]})\n\u001B[32m    376\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m * \u001B[32m50\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 320\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, X_train, y_train, X_val, y_val, epochs, batch_size, lr)\u001B[39m\n\u001B[32m    317\u001B[39m y_batch = y_train_shuffled[start_idx:end_idx]\n\u001B[32m    319\u001B[39m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m320\u001B[39m probs, cache = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    322\u001B[39m \u001B[38;5;66;03m# Compute loss\u001B[39;00m\n\u001B[32m    323\u001B[39m loss = -np.mean(np.log(probs[\u001B[38;5;28mrange\u001B[39m(batch_size), y_batch] + \u001B[32m1e-8\u001B[39m))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 211\u001B[39m, in \u001B[36mLSTMNetwork.forward\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    209\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(seq_len):\n\u001B[32m    210\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.embeddings[X[:, t]]\n\u001B[32m--> \u001B[39m\u001B[32m211\u001B[39m     h, c, cache = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlstm_cell\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    212\u001B[39m     caches.append(cache)\n\u001B[32m    214\u001B[39m logits = np.dot(h, \u001B[38;5;28mself\u001B[39m.Wy) + \u001B[38;5;28mself\u001B[39m.by\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 87\u001B[39m, in \u001B[36mLSTMCell.forward\u001B[39m\u001B[34m(self, x, h_prev, c_prev)\u001B[39m\n\u001B[32m     85\u001B[39m i = sigmoid(np.dot(concat, \u001B[38;5;28mself\u001B[39m.Wi) + \u001B[38;5;28mself\u001B[39m.bi)  \u001B[38;5;66;03m# Input gate\u001B[39;00m\n\u001B[32m     86\u001B[39m c_tilde = tanh(np.dot(concat, \u001B[38;5;28mself\u001B[39m.Wc) + \u001B[38;5;28mself\u001B[39m.bc)  \u001B[38;5;66;03m# Candidate cell state\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m87\u001B[39m o = \u001B[43msigmoid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconcat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mWo\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbo\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Output gate\u001B[39;00m\n\u001B[32m     89\u001B[39m c = f * c_prev + i * c_tilde  \u001B[38;5;66;03m# New cell state\u001B[39;00m\n\u001B[32m     90\u001B[39m h = o * tanh(c)  \u001B[38;5;66;03m# New hidden state\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 52\u001B[39m, in \u001B[36msigmoid\u001B[39m\u001B[34m(x)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msigmoid\u001B[39m(x):\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[32m1\u001B[39m / (\u001B[32m1\u001B[39m + np.exp(-\u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m500\u001B[39;49m\u001B[43m)\u001B[49m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Coding_Enviroment\\NLP\\Ex1\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2330\u001B[39m, in \u001B[36mclip\u001B[39m\u001B[34m(a, a_min, a_max, out, min, max, **kwargs)\u001B[39m\n\u001B[32m   2326\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mmin\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np._NoValue \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mmax\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np._NoValue:\n\u001B[32m   2327\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mPassing `min` or `max` keyword argument when \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   2328\u001B[39m                      \u001B[33m\"\u001B[39m\u001B[33m`a_min` and `a_max` are provided is forbidden.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m2330\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mclip\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma_min\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma_max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Coding_Enviroment\\NLP\\Ex1\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57\u001B[39m, in \u001B[36m_wrapfunc\u001B[39m\u001B[34m(obj, method, *args, **kwds)\u001B[39m\n\u001B[32m     54\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, *args, **kwds)\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbound\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m     59\u001B[39m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[32m     60\u001B[39m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     64\u001B[39m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[32m     65\u001B[39m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[32m     66\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Coding_Enviroment\\NLP\\Ex1\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:115\u001B[39m, in \u001B[36m_clip\u001B[39m\u001B[34m(a, min, max, out, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m um.maximum(a, \u001B[38;5;28mmin\u001B[39m, out=out, **kwargs)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mum\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclip\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dd88e4a2721b8bd8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
